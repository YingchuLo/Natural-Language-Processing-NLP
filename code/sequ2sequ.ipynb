# Part II: Sequence to Sequence Models in tensorflow
Task: Language Modeling
Today we will experiment with using tensorflow to build a very simple seq2seq structure for language modeling.

Definition
We input a sequence of words/characters to an RNN so that it can learn the probability distribution of the next word/character in the sequence given the history of previous characters. This will then allow us to generate text one unit at a time.

We will use 全唐诗 as our training data, and try to generate new poems later!

Check the content
I have already uploaded the poem text to this server. We need to first do some preprocessing.

Confirm the correctness of preprocessing

When you deal with your own dataset, you have to write your own preprocessing procedure. There are all kinds of noise in text data. Remember to check the correctness of the preprocessing!

The format of our input data is like this:

(optional title + ":")poem

We will use only the poem part and not the title.

However, some special cases like:

河鱼未上冻，江蛰已闻雷。（见《纬略》）
□□□□□
We need some preprocessing as mentioned in our last codelab.

Remove title
Remove spaces
Remove empty symbols
Replace other symbols
Finally, we will randomize them.

In [0]:
import re
import numpy as np

data_filename ='poetry.txt'
poems = []
with open(data_filename, "r") as in_file:
  for line in in_file.readlines():
    line = line.strip()
    # find title if exists
    if ':' in line:
      line = line.split(':')
    # some poems are empty
    if len(line) == 2:
      poem = line[1]
    else:
      continue
    # discard if contains special symbols
    if re.search(r'[(（《_□]', poem):
      continue
    # discard if too short or too long
    if len(poem) < 5 or len(poem) > 40:
      continue
    # remove symbols
    poem = re.sub(u'[，。]','',poem)
    poems.append(poem)

poems = np.random.permutation(poems)
We select 5 poems as our test set.
